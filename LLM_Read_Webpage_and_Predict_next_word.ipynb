'''
Use of LLMs to read from a webpage as the training data. Predicts the next 
word of given input.

'''

!pip install -q torch numpy

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import string
import requests
from bs4 import BeautifulSoup
import string

# Sample text from a Guelph-related article or general text
def get_live_text(url):
    # 1. Fetch the webpage content
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Failed to load page. Error: {response.status_code}")
        return ""
    
    # 2. Parse the HTML
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # 3. Extract text from paragraph tags (most articles live in <p> tags)
    paragraphs = soup.find_all('p')
    text = " ".join([p.get_text() for p in paragraphs])
    
    # Clean up excess whitespace
    return " ".join(text.split())

# --- Update your training data here ---
url = "https://www.uoguelph.ca/continuing-studies/" # Example URL
article_text = get_live_text(url)

print(f"Successfully scraped {len(article_text.split())} words from the site.")


# 1. Tokenize and Build Vocabulary
tokens = article_text.lower().replace('.', '').split()
vocab = sorted(list(set(tokens)))
word_to_idx = {word: i for i, word in enumerate(vocab)}
idx_to_word = {i: word for i, word in enumerate(vocab)}
vocab_size = len(vocab)

# 2. Create Sequences (X) and Targets (y)
# Example: Input ["the", "university"] -> Target "of"
sequence_length = 2
X, y = [], []
for i in range(len(tokens) - sequence_length):
    seq = tokens[i:i + sequence_length]
    label = tokens[i + sequence_length]
    X.append([word_to_idx[w] for w in seq])
    y.append(word_to_idx[label])

X = torch.tensor(X)
y = torch.tensor(y)


class NextWordLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super(NextWordLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.ModuleList([nn.LSTM(embed_dim, hidden_dim, batch_first=True)])
        self.fc = nn.Linear(hidden_dim, vocab_size)
        
    def forward(self, x):
        embedded = self.embedding(x)
        # Use the final hidden state to predict the next word
        lstm_out, _ = self.lstm[0](embedded)
        last_time_step = lstm_out[:, -1, :]
        logits = self.fc(last_time_step)
        return logits

# Hyperparameters
embed_dim = 16
hidden_dim = 32
model = NextWordLSTM(vocab_size, embed_dim, hidden_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training Loop
for epoch in range(100):
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()

# Prediction Function

def clean_input(text):
    # Remove punctuation and make lowercase to match our training data
    text = text.lower().translate(str.maketrans('', '', string.punctuation))
    return text.split()

'''
def predict_from_prompt():
    model.eval()
    print("\n--- Next Word AI Initialized ---")
    print(f"(Note: Please use words from the article: {', '.join(vocab[:5])}...)")
    
    while True:
        user_input = input("\nEnter a sentence (or 'q' to quit): ")
        
        if user_input.lower() == 'q':
            break
            
        words = clean_input(user_input)
        
        # We need at least the 'sequence_length' to make a prediction
        if len(words) < sequence_length:
            print(f"Error: Please enter at least {sequence_length} words.")
            continue
            
        try:
            # Take the last 'sequence_length' words from the user's prompt
            last_words = words[-sequence_length:]
            input_idxs = [word_to_idx[w] for w in last_words]
            input_tensor = torch.tensor([input_idxs])
            
            with torch.no_grad():
                output = model(input_tensor)
                # Get the index of the highest probability word
                predicted_idx = torch.argmax(output, dim=1).item()
                prediction = idx_to_word[predicted_idx]
                
            print(f"AI suggests the next word is: **{prediction}**")
            
        except KeyError as e:
            print(f"Error: The word '{e.args[0]}' is not in my vocabulary!")

'''
def predict_from_prompt():
    model.eval()
    print(f"\n--- AI is ready (Context Window: {sequence_length} words) ---")
    
    while True:
        user_input = input("\nEnter your prompt: ")
        if user_input.lower() == 'q': break
        
        words = clean_input(user_input)
        
        # Check if user provided enough words for the model's memory
        if len(words) < sequence_length:
            print(f"Error: I need at least {sequence_length} words to think!")
            continue
            
        try:
            # DYNAMIC SLICE: Always takes the LAST 'sequence_length' words
            # This allows you to type a sentence of ANY length.
            last_words = words[-sequence_length:] 
            
            input_idxs = [word_to_idx[w] for w in last_words]
            input_tensor = torch.tensor([input_idxs])
            
            with torch.no_grad():
                output = model(input_tensor)
                predicted_idx = torch.argmax(output, dim=1).item()
                print(f"Next word: {idx_to_word[predicted_idx]}")
                
        except KeyError as e:
            print(f"Error: Word {e} not found in the website text.")


# Run the interactive prompt
predict_from_prompt()
