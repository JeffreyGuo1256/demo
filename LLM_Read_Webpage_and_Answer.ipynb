# Run this, then go to Runtime > Restart Session
!pip install -q --no-warn-conflicts chromadb sentence-transformers requests beautifulsoup4

import torch
import requests
import chromadb
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer

# 1. SETUP
print("Initializing AI...")
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# 2. THE SCRAPER (with URL Sanitizer)
def scrape_university_page(url):
    # Add https:// if the user forgot it
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status() # Check if page exists
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Grab paragraphs that have actual content (more than 30 characters)
        paragraphs = [p.get_text().strip() for p in soup.find_all('p') if len(p.get_text()) > 30]
        return paragraphs
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return []

# 3. DATABASE SETUP
client = chromadb.Client()
try:
    client.delete_collection("web_knowledge_bot")
except:
    pass
collection = client.create_collection(name="web_knowledge_bot")

# 4. DATA INGESTION
urls = ["www.uoguelph.ca", "https://www.uoguelph.ca/lang/bcomm/course-selection-faq/"] # <--- Ensure https:// is here!

print(f"Feeding knowledge from {len(urls)} websites...")
for url in urls:
    chunks = scrape_university_page(url)
    if not chunks:
        continue
        
    # Generate vectors for each paragraph and store them
    embeddings = model.encode(chunks).tolist()
    collection.add(
        embeddings=embeddings,
        documents=chunks,
        ids=[f"{url}_{i}" for i in range(len(chunks))]
    )
print("Database ready!")

# 5. THE CHAT FUNCTION
def chat():
    print("\nAsk me anything about the websites I just read! (Type 'q' to quit)")
    while True:
        query = input("\nYour Question: ").strip()
        if query.lower() in ['q', 'quit']: break
        
        # Search the database
        query_vec = model.encode(query).tolist()
        results = collection.query(query_embeddings=[query_vec], n_results=1)
        
        if results['distances'][0][0] > 1.2: # Similarity threshold
            print("Bot: I'm not sure about that. Try asking something else.")
        else:
            print(f"Bot: {results['documents'][0][0]}")

chat()

